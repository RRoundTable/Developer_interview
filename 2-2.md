## Gradient Descent에 대해서 쉽게 설명한다면?

gradient descent는 optimal한 값을 한 번에 찾는 것이 아니라 한 step마다 확인하고 W 값을 변경하여 optimal한 지점을 찾는다.

![gradient_descent](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8KfGSvv5gVeC3FkWeOm16lGSlcMfUetuIsRb9rbA8AGF-QndUsQ)

### 왜 꼭 Gradient를 써야 할까?

optimization이란, 특정 함수에 대해서 최대 혹은 최소가 되는 상태를 해석하는 문제이다.

- 사실 optimization 방법에는 여러가지 방법이 있다.
- Newton's method : Methods that evaluate Hessians (or approximate Hessians, using finite differences)
  단 한번의 계산으로 optimal point를 알 수 있다. 하지만, hessians matrix를 구하는 것은 현재의 computing power로는 불가능하다고 한다.
- 반면에, gradient descent는 한 point마다 loss값을 구하고 그 값에 대해서 loss가 커졌으면 W=W+gradient*lr / loss가 작아지면 W=W-gradient*lr
- 따라서 loss함수의 gradient만 구할 수 있다면, 여러번의 실험을 통해서 optimal한 값을 추정할 수 있다.

### 그 그래프에서 가로축과 세로축 각각은 무엇인가?

![gradient_descent](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8KfGSvv5gVeC3FkWeOm16lGSlcMfUetuIsRb9rbA8AGF-QndUsQ)

x축은 : Weight의 value를 의미하고
y축은 : loss를 의미한다

### 실제 상황에서는 그 그래프가 어떻게 그려질까?

질문의 의도를 잘 모르겠다.

### GD 중에 때때로 Loss가 증가하는 이유는?

두 가지 이유가 있을 수 있다.

- learning rate의 크기

  lr이 적절하지 못하게 크다면 optimal point로 수렴하다가 optimal point를 넘을 수 있다.

- SGD(stochastic Gradient Descent)의 영향

  SGD란 모든 데이터를 넣고 학습 시키는 것이 아니라 batch_size만큼 추출하여 train하는 것을 의미한다. 
  따라서, 뽑히는 데이터의 분포가 loss값의 영향을 줄 수 있다.

### 중학생이 이해할 수 있게 더 쉽게 설명 한다면?

쉽게 설명한다면, 내가 금고털이범이라고 가정한다면 금고의 번호(optimal point)를 맞출 때 소리(loss)를 듣고 스위치를 돌릴 방향(W)을 결정한다. 
원하는 소리에 가까우면 그 방향 그대로 돌리고 원하는 소리가 아니라면 반대방향으로 돌린다.

원하는 소리가 나다가 갑자기 이상한 소리가 나면 다시 반대방향으로 돌려보기도 한다.(loss값이 갑자기 증가)



### Back Propagation에 대해서 쉽게 설명 한다면?

Deeplearning은 수학적으로 표현하면 '합성 함수'라고 생각한다.


함성함수의 chain rule(연쇄 작용)인데, 간단히 설명하면 '나비 효과'와 비슷한 것이다.
![chain_rule](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRFL1n4m0et2yNoOyGZH0t-spIR860pUylCEg65lxQCIs1zf7nh)

'나의 작은 행동 하나가 결과를 어떻게든 바꿀 수 있다.'

위의 그래프에서 loss의 gradient(loss/dW)는 사실 chain rule의 결과이다. 

따라서 dW는 여러가지 함수로 구성되는데 W가 변화할 때 마다 그 구성요소들이 함께 변한다.
 
정리하자면, back propagation은 chain rule의 역순으로 보면 될 것이다.

![backpropagation](https://cdn-images-1.medium.com/max/1600/1*q1M7LGiDTirwU-4LcFq7_Q.png)

```python

w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # sigmoid함수의 미분값 local gradient

# backprop into w : dW는 W에 대한 전체 함수의 미분값을 의미한다. 여기서 X는 local gradient를 의미함.
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot]
# we're done! we have the gradients on the inputs to the circuit

```
