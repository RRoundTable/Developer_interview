
## Word2Vec의 원리는?

![word2vec](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnCkLNbTEG1SRCGVe37PrxjhG0PzhzoOgWluQQBxrDDwRUXozC)
![word2vec2](http://adventuresinmachinelearning.com/wp-content/uploads/2017/07/Word2Vec-softmax-676x381.jpg)

컴퓨터는 word를 인식할 수 없다. word 데이터를 처리하기 위해서는 word와 number를 대응시켜서 처리한다.
예를 들어서,{ 감자:0, 고구마:1, 호박:2} 이런식으로 처리하는 것이다.

하지만,

이런식으로 데이터를 다루는 것은 'discrete'한 공간에서 데이터를 다루는 것이다.(이산공간)
이산공간에서 데이터를 다루면 데이터간 어떤 의미가 있는지 살피기 힘들다.

한 마디로, word2vec은 discrete한 data를 continuous한 공간에서 다루고자하는 것이다.

### 그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는?

### 그 그림에서 오른쪽 파라메터들의 의미는 무엇일까?

### 남자와 여자가 가까울까? 남자와 자동차가 가까울까?

질문의 의미는 유사한 역할을 하는 반대되는 word가 의미적으로 더 가까울 것인가 아니면 word와 관련된 단어가 더 가까울 것인가로 받아들여도 될 것 같다.

### 번역을 Unsupervised로 할 수 있을까?

unsupervised learning은, labeling 하지 않은 데이터로 가지고 결과를 도출해야 한다.

번역이라는 task는 한 문장이 들어오면 encoding과 decoding과정을 거친다고 볼 수 있다.

이렇게 생각하면, decoding한 결과와 translation결과를 서로 비교해야 loss를 측정할 수 있다. 이는 곧 unsupervised learning으로는 불가능하다는 결론에 이른다.

- 우선, word embedding은 unsupervised learning으로 진행할 수 있다.

  문장들의 집합만으로 word간의 어떤 관계가 있는지 살펴볼 수 있다.

이는 skip-gram이나 cbow 같은 방법을 사용해서
