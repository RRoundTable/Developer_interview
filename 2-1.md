### 2. 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

sigmoid와 Relu는 모두 activation function이다.
sigmoid란 {\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}.}
![sigmoid](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHSk3U3OizdXDWN198r0EhU-_1C4slOQwWlIhUu0uPxWMxiurL)

Relu란 {\displaystyle f(x)=x^{+}=\max(0,x)}


![Relu](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/220px-Rectifier_and_softplus_functions.svg.png)

- Non-Linearity라는 말의 의미와 그 필요성은?

- ReLU로 어떻게 곡선 함수를 근사하나?

- ReLU의 문제점은?

- Bias는 왜 있는걸까?)
