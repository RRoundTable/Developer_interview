### 2. 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

sigmoid와 Relu는 모두 activation function이다.
sigmoid란 

![sigmoid](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHSk3U3OizdXDWN198r0EhU-_1C4slOQwWlIhUu0uPxWMxiurL)

Relu란 

![Relu](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/220px-Rectifier_and_softplus_functions.svg.png)

sigmoid보다 Relu를 많이 사용하는 이유는 'vanishing gradient'라는 현상 때문이다. 

![vanishing gradient](https://t1.daumcdn.net/cfile/tistory/221E5750579F7BBD1B)

backpropagation을 진행할 때, gradient 값이 줄어들게 되어서 train이 잘 안되는 현상을 의미한다. 
이런 현상이 발생하는 이유는 sigmoid의 gradient 절대값이 0~1사이 값이 layer별로 나올 경우 backpropation의 마지막 단계에서는 gradient가 거의 사라지는 효과가 나타난다.

이를 해결하기 위해서 Relu를 사용하는데 Relu의 gradient의 값은 특정한 영역(0보다 클 때)에서 일정한 값 (=1)을 가지고 있기 때문에 위의 문제를 해결 할 수 있다.

- Non-Linearity라는 말의 의미와 그 필요성은?

non-linearity란, 비선형적인 것 즉, 직선적인 특성을 가지고 있지 않은 것을 의미한다.
이런 특성이 필요한 이유는 'logistic regression'에서 잘 들어난다.

binary한 classification을 진행하기 위해서는 linear regression과는 다른 접근 방법이 필요한데, 바로 일정기준이 지나면 1로 일정기준을 못 미치면 0으로 판단할 수 있어야한다. 즉, linear한 함수가 아니라는 것이다.

activation function도 이와 유사한 가정을 가진다. activation function은 다음 노드에 input 값을 부여할지 말지 판단하는 classification 문제로 볼 수 있다. 따라서 non-linearity한 특성이 필요하다.

- ReLU로 어떻게 곡선 함수를 근사하나?

- ReLU의 문제점은?

- Bias는 왜 있는걸까?)
