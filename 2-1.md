### 1. 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

sigmoid와 Relu는 모두 activation function이다.
sigmoid란 

![sigmoid](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHSk3U3OizdXDWN198r0EhU-_1C4slOQwWlIhUu0uPxWMxiurL)

Relu란 

![Relu](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/220px-Rectifier_and_softplus_functions.svg.png)

sigmoid보다 Relu를 많이 사용하는 이유는 'vanishing gradient'라는 현상 때문이다. 

![vanishing gradient](https://t1.daumcdn.net/cfile/tistory/221E5750579F7BBD1B)

backpropagation을 진행할 때, gradient 값이 줄어들게 되어서 train이 잘 안되는 현상을 의미한다. 
이런 현상이 발생하는 이유는 sigmoid의 gradient 절대값이 0~1사이 값이 layer별로 나올 경우 backpropation의 마지막 단계에서는 gradient가 거의 사라지는 효과가 나타난다.

이를 해결하기 위해서 Relu를 사용하는데 Relu의 gradient의 값은 특정한 영역(0보다 클 때)에서 일정한 값 (=1)을 가지고 있기 때문에 위의 문제를 해결 할 수 있다.

- Non-Linearity라는 말의 의미와 그 필요성은?

non-linearity란, 비선형적인 것 즉, 직선적인 특성을 가지고 있지 않은 것을 의미한다.
이런 특성이 필요한 이유는 'logistic regression'에서 잘 들어난다.

binary한 classification을 진행하기 위해서는 linear regression과는 다른 접근 방법이 필요한데, 바로 일정기준이 지나면 1로 일정기준을 못 미치면 0으로 판단할 수 있어야한다. 즉, linear한 함수가 아니라는 것이다.

activation function도 이와 유사한 가정을 가진다. activation function은 다음 노드에 input 값을 부여할지 말지 판단하는 classification 문제로 볼 수 있다. 따라서 non-linearity한 특성이 필요하다.

- ReLU로 어떻게 곡선 함수를 근사하나?

함수를 근사한다는 것은 함수를 추정하여 동일한 input을 넣었을 때 output이 유사하게 나오도록 조정하는 과정이다.

Relu는 함수의 각 노드들의 weight을 어떻게 부여할지 결정하는 역할을 하는데, sigmoid와 다르게 activation되는 값과 backpropagation되는 정도를 동일하게 가져간다. 

- ReLU의 문제점은?

ReLu의 문제점은 0보다 작은 activation값에 대해서는 모두 동일하게 0으로 처리한다는 것이다. 이는 정보의 일정부분을 활용하지 않는 것으로 볼 수 있는데, 이를 보완하는 activation funcion으로는 'selu'가 있으며 이 function은 0보다 적은 activation값에도 정보를 활용한다.

다만, 해결하는 문제에 따라서 activation function을 고려하는 것이 근복적으로 옳은 선택이며, 이는 현재는 경험적으로 알 수 밖에 없다.

- Bias는 왜 있는걸까?

통계학에서 bias는 variation과 함께 논해야한다.

왜냐하면 bias와 var는 항상 trade-off관계를 가지고 있기 때문이다. 게다가 'capacity'라는 개념도 함께 고려해야한다.

![bias-var-capacity](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTrs2cK6grmB96GN5oAlZhRiEMLUfeKtigMfJo7JaCInMmLbwjo)

bias란, '치우쳐짐' 정도로 이해하면 되는데 실제값과 예측값의 편차를 의미한다. (이 정도는 틀렸겠구나 하고 포기하는 정도로 생각해도 될 것 같다.)

var은 '퍼짐' 정도로 이해하면 되는데 이는 곧 얼마나 데이터를 정확하게 예측할 수 있는 지표가 된다. (분산이 높다면, 정확도는 떨어진다.)

capacity(모델 복잡도)는 이 모델을 얼마나 복잡하고 깊게 가져갈 것인가를 나타냅니다. 결국 위의 이미지에서 볼 수 있듯이 적당한 모델 복잡도를 가져야 overfitting을 피할 수 있습니다.

따라서 bias는 결국 모델 복잡도를 결정하는 요인이 됩니다.

이 trade-off관계는 mean-squared-error를 살펴보면 확실히 알 수 있다.

![MSE](https://modulabs-biomedical.github.io/assets/images/posts/2018-01-25-Bias_vs_Variance/1.jpg)

