### 2. 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?

sigmoid와 Relu는 모두 activation function이다.
sigmoid란 

![sigmoid](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHSk3U3OizdXDWN198r0EhU-_1C4slOQwWlIhUu0uPxWMxiurL)

Relu란 

![Relu](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Rectifier_and_softplus_functions.svg/220px-Rectifier_and_softplus_functions.svg.png)

sigmoid보다 Relu를 많이 사용하는 이유는 'vanishing gradient'라는 현상 때문이다. 

![vanishing gradient](https://t1.daumcdn.net/cfile/tistory/221E5750579F7BBD1B)

backpropagation을 진행할 때, gradient 값이 줄어들게 되어서 train이 잘 안되는 현상을 의미한다. 
이런 현상이 발생하는 이유는 sigmoid의 gradient 절대값이 0~1사이 값이 layer별로 나올 경우 backpropation의 마지막 단계에서는 gradient가 거의 사라지는 효과가 나타난다.

이를 해결하기 위해서 Relu를 사용하는데 Relu의 gradient의 값은 특정한 영역(0보다 클 때)에서 일정한 값 (=1)을 가지고 있기 때문에 위의 문제를 해결 할 수 있다.

- Non-Linearity라는 말의 의미와 그 필요성은?

- ReLU로 어떻게 곡선 함수를 근사하나?

- ReLU의 문제점은?

- Bias는 왜 있는걸까?)
