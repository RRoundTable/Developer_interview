## Auto Encoder에 대해서 아는대로 얘기하라

![autoencoder](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTNKtx8ZT2pv_AqjGTlgpVb4An7JlxAR-YVF3jY5TJqdXzTy7cG)

Auto Encoder는 input data를 더 낮은 차원으로 축소 시킨 후, 다시 복원하는 과정을 거친다. 

이때, 차원을 축소하는 이유는 input data를 구성하는 의미있는 요인을 학습하기 위해서 이다. 

학습하는 과정에서 사용하는 Loss function은 L2 loss이다.

input data와 reconstructed input data간의 l2 loss를 감소시키는 방향으로 학습한다. 이 때, input data는 labeling이 필요없다!

### MNIST AE를 TF나 Keras등으로 만든다면 몇줄일까

layer의 수에 따라 다소 차이는 있겠으나, 기본적인 구성은 다음과 같다.

1. import module

2. parameter(learning rate, batch_size, 등등)

3. layer 정의

4. encoder 함수 정의

5. decoder 함수 정의

6. loss 정의

7. 학습


- MNIST data

  class : 0,1,2,3,4,5,6,7,8,9
  
  input data dimension : 28*28 (data 하나 당)

### MNIST에 대해서 임베딩 차원을 1로 해도 학습이 될까?

'학습이 된다' 라는 것은 loss를 의미있게 줄여나갈 수 있냐는 질문으로 받아들이겠다.

이는 곧 1차원의 vector가 mnist의 data의 의미있는 feature를 추출할 수 있냐는 것인데,

1차원의 vector는 직선의 공간만 표현할 수 있다.

![dimension](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQzFvFpS-UdohQWvgmA5-v0bAOMwX9vlbmfihDYMfjdNEqbiR4x9w)

이 정도의 정보량으로 decoder가 MNIST이미지를 복원할 수 있을 것인가?

-  우선 현실적으로는 불가능 할 것이라고 생각한다.

-  한가지 클래스만 가지고 학습하고, 정말 복잡하고 깊은 network로 이루어진 encoder, decoder라면, 가능할 것 같다. 
  [0]이라는 정보만으로, MNIST '0'의 이미지를 복원할 수 있을 것 같다. 그리고, 같은 '0'이어도 [0.002]이면 조금 찌그러진 0으로 표현할 수 있지 않을까 생각이 든다.

- 두 가지 이상의 class라면, 불가능 할 것이라고 생각한다.

위에 언급했듯이 하나의 class만을 학습시킨다면, 가능할 여지가 있다고 본다. 

하지만, class가 두 가지라면 이야기는 달라진다.0과 1 class가 있다고 가정해보자.

위에서는 [0.0002]가 0의 모양에 영향을 줄 수 있다고 했다. 하지만, [0.5]는 어떤 의미를 가질 것인가.

class 0과 1은 서로 유사한 형태라고 보기 힘들다. 둘은 continuous한 1차원 vector로 나타낸다면 많은 정보의 손실이 일어날 것이다.(의미가 없어질 만큼)


### 임베딩 차원을 늘렸을 때의 장단점은?

임베딩 차원을 늘린다면, 의미있는 feature를 더 많이 추출할 수 있을 것이다.

차원이 높아진다면, 그만큼 더 많은 정보를 포함할 수 있다는 의미이다.

하지만, 차원수가 높아진다면, 필요없는 정보까지 내포할 가능성도 있다.

따라서 적절한 임베딩차원을 '실험'을 통해서 적절하게 유추하는 것이 필요하다.

### AE 학습시 항상 Loss를 0으로 만들수 있을까?

임베딩 차원을 input data의 차원과 동일하게 만든다면, 가능하겠다.

하지만, 일반적인 AE를 고려했을 때, AE의 LOSS함수가 L2의 형태를 가지기 때문에, 모든 정보의 손실이 없다고는 하지 못하겠다.

### VAE는 무엇인가?

VAE는 Variation Auto Encoder의 약자로 AE에 확률에 개념을 부여한 것이다.

![VAE1](https://1.bp.blogspot.com/-0asYZ9AJH-M/WOjCRLXyz7I/AAAAAAAABmI/8E36K71xD0kjSVjDWajfz-EUJHl8LqGSQCK4B/s1600/vae_1.png)

VAE는 MLP접근방법을 따르며, Explicit density의 특성을 가진다.

Explicit density란, 확률분포에 대해서 sampling이 아니라 명확한 정의를 통해서 접근하는 형식을 의미합니다.

(반면에, implicit density는 sampling을 통해서 확률분포를 정의합니다.)

또한, VAE의 확률분포는 intractable하여 approximate과정을 통해서 구한다.

approximate과정은 크게 Monte Carlo approximation과 Variational Inference로 이루어져 있는데,

(Variational Inference에 대해서는 더 공부가 필요하다.)


![VAE2](https://image.slidesharecdn.com/mizunofinal-161027093055/95/anomaly-detection-by-adgm-lvae-3-638.jpg?cb=1477647360)

VAE는 encoder와 decoder가 모두 neural network로 이루어져 있으며, latent variable을 통해서 data를 생성한다.(재구축과 다른 개념)
